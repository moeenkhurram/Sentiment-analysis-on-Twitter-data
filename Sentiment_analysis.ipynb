{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "twitter",
      "language": "python",
      "name": "twitter"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Sentiment analysis",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeenkhurram/Sentiment-analysis-on-Twitter-data/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNMWkfxPJCPO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud \n",
        "\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Global Parameters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeUtpoq_OOqH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WsC7dW6JCPs"
      },
      "source": [
        "# About Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8tfyPbBJCPu"
      },
      "source": [
        "Dataset: [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/kazanova/sentiment140)\n",
        "\n",
        "This is the sentiment140 dataset.\n",
        "It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\n",
        "It contains the following 6 fields:\n",
        "\n",
        "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "ids: The id of the tweet ( 2087)\n",
        "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "user: the user that tweeted (robotickilldozr)\n",
        "text: the text of the tweet (Lyx is cool)\n",
        "The official link regarding the dataset with resources about how it was generated is here\n",
        "The official paper detailing the approach is here\n",
        "\n",
        "According to the creators of the dataset:\n",
        "\n",
        "\"Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search\"\n",
        "\n",
        "citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW1KrQ1vJCPv"
      },
      "source": [
        "colnames=['target','ids' ,'date','flag','user','text']\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/archive.zip\",encoding=DATASET_ENCODING, names=colnames, header=None)\n",
        "df = df.sample(int(len(df)/2)).reset_index(drop=True)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl25vb-sJCPz"
      },
      "source": [
        "df= df.iloc[:,[0,-1]]\n",
        "df.columns = ['sentiment','tweet']\n",
        "#df = pd.concat([df.query(\"sentiment==0\").sample(20000,  random_state=7),df.query(\"sentiment==4\").sample(20000, random_state=7)])\n",
        "df.sentiment = df.sentiment.map({0:0, 4:1})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVW-ftoYTfIt"
      },
      "source": [
        "def preprocess_tweet_text(tweet):\n",
        "    tweet.lower()\n",
        "    # Remove urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "    # Remove retweets:\n",
        "    tweet = re.sub(r'RT : ', '', tweet)\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
        "    \n",
        "    #ps = PorterStemmer()\n",
        "    #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
        "    \n",
        "    return \" \".join(lemma_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_BbibTGJCP0"
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(preprocess_tweet_text)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dutQTLcCJCP3"
      },
      "source": [
        "# Creating a word cloud\n",
        "words = ' '.join([tweet for tweet in df['tweet']])\n",
        "wordCloud = WordCloud(width=1200, height=800).generate(words)\n",
        "\n",
        "plt.imshow(wordCloud)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ow33-ZJCP5"
      },
      "source": [
        "# TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chlJN7whPf7b"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob import Blobber\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "\n",
        "blobber = Blobber(analyzer=NaiveBayesAnalyzer())\n",
        "\n",
        "blob = blobber(\"The movie was good, I do not want to watch it agian\")\n",
        "print(blob.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJSyl-OBWPds"
      },
      "source": [
        "df1 = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqsDCbdGJCQN"
      },
      "source": [
        "def getTextSubjectivity(txt):\n",
        "    return TextBlob(txt).sentiment.subjectivity\n",
        "\n",
        "def getTextPolarity(txt):\n",
        "    return TextBlob(txt).sentiment.polarity\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPJLKT2NJCQO"
      },
      "source": [
        "df1['Subjectivity'] =     df1['tweet'].apply(getTextSubjectivity)\n",
        "df1['Polarity']     =     df1['tweet'].apply(getTextPolarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKTN-x3NeJHS"
      },
      "source": [
        "# negative, nautral, positive analysis \n",
        "def Sentiments_Score(tweet):\n",
        "    if tweet < 0:\n",
        "        return \"Negative\"\n",
        "    elif tweet == 0:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\"    \n",
        "\n",
        "df1['Predicition_Textblob'] = df1['Polarity'].apply(Sentiments_Score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFUe5Xh5eI1D"
      },
      "source": [
        "df1.sample(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34ypxetYJCQS"
      },
      "source": [
        "plt.bar(df1.groupby('Predicition_Textblob').count().index.values, df1.groupby('Predicition_Textblob').size().values) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEyDDhtvJCQT"
      },
      "source": [
        "## Vader Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdLJj8HXJCQT"
      },
      "source": [
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P4dgLqNJCQV"
      },
      "source": [
        "df2= df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dBbH9GiJCQW"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R4WwUqsJCQX"
      },
      "source": [
        "df2['Predicition_Vader']=''\n",
        "\n",
        "def Vader_Sentiment(df2):\n",
        "    df2[\"neg\"] = df2[\"tweet\"].apply(lambda x:analyser.polarity_scores(x)[\"neg\"])\n",
        "    df2['neu'] = df2['tweet'].apply(lambda x:analyser.polarity_scores(x)['neu'])\n",
        "    df2['pos'] = df2['tweet'].apply(lambda x:analyser.polarity_scores(x)['pos'])\n",
        "    df2['compound'] = df2['tweet'].apply(lambda x:analyser.polarity_scores(x)['compound'])\n",
        "    \n",
        "    # negative, nautral, positive analysis \n",
        "    df2.loc[df2.compound>0,'Predicition_Vader']='Positive'\n",
        "    df2.loc[df2.compound==0,'Predicition_Vader']='Neutral'\n",
        "    df2.loc[df2.compound<0,'Predicition_Vader']='Negative'\n",
        "\n",
        "    return df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtgSzbn2JCQZ"
      },
      "source": [
        "df2 = Vader_Sentiment(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8lCOVJqJCQg"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIe1q7FhJCQj"
      },
      "source": [
        "plt.bar(df2.groupby('Predicition_Vader').count().index.values,  df2.groupby('Predicition_Vader').size().values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyMZnVgwJCQn"
      },
      "source": [
        "df3=df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9oEkj_Xf2T"
      },
      "source": [
        "df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FNQWQGbOVgH"
      },
      "source": [
        "# 0: Negative\n",
        "# 1: Positive\n",
        "\n",
        "print(df3.tweet[100])\n",
        "print(df3.sentiment[100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLawHgKNOkJ2"
      },
      "source": [
        "training_size = int(len(df3.tweet) * 0.8)\n",
        "\n",
        "X_train = df3.tweet[0: training_size].values\n",
        "X_test = df3.tweet[: training_size].values\n",
        "\n",
        "y_train = df3.sentiment[0: training_size].values\n",
        "y_test = df3.sentiment[: training_size].values\n",
        "\n",
        "# Put labels into list to use later:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiX07mE6kP1m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67d2IhREPXEn"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer_obj = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer_obj.fit_on_texts(X_train) \n",
        "\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "max_length = 100\n",
        "\n",
        "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznSAbx2PZt7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "embedding_dim = 16\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim , input_length=max_length))\n",
        "model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(embedding_dim, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQG8_u4ryXQ"
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BDG5BxhjPG"
      },
      "source": [
        "\n",
        "num_epochs=5\n",
        "history = model.fit(X_train_pad, y_train, \n",
        "                    epochs = num_epochs,\n",
        "                    batch_size=256,\n",
        "                    validation_data=(X_test_pad, y_test),\n",
        "                    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, mode='auto')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNmw2rJ8x_rM"
      },
      "source": [
        "# Decrease because the early stopping\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ9VN5wqzSS2"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slc1F_ze0X8v"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e435qaMKzW0F"
      },
      "source": [
        "# Use the model to predict Tweeets  \n",
        "\n",
        "Test_Tweet =  ['I love this phone']\n",
        "\n",
        "print(Test_Tweet) \n",
        "\n",
        "# Create the sequences\n",
        "\n",
        "Test_Tweet_sequences = tokenizer_obj.texts_to_sequences(fake_reviews)\n",
        "Test_Tweet_padded = pad_sequences(Test_Tweet_sequences, maxlen=max_length, padding=\"post\" )           \n",
        "\n",
        "prediciton = model.predict(Test_Tweet_padded, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwq9hGFn0luU"
      },
      "source": [
        "if(np.argmax(prediciton) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(prediciton) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP1vkJXM5Cmq"
      },
      "source": [
        "np.round(np.argmax(prediciton), decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BQjpPMiRRZr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}